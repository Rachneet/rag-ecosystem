{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d5b5a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36f49d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from token import OP\n",
    "from langchain_huggingface import (\n",
    "    HuggingFaceEndpointEmbeddings,\n",
    "    HuggingFaceEndpoint,\n",
    "    ChatHuggingFace\n",
    ")\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def get_hf_llm(model_name: str, model_kwargs: Optional[dict]) -> ChatHuggingFace:\n",
    "\n",
    "    if not model_kwargs:\n",
    "        model_kwargs = {\n",
    "            \"max_new_tokens\": 1024,\n",
    "        }\n",
    "    model = HuggingFaceEndpoint(\n",
    "        model=model_name,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    return ChatHuggingFace(llm=model)\n",
    "\n",
    "# test \n",
    "llm = get_hf_llm(\"meta-llama/Llama-3.1-8B-Instruct\", model_kwargs={})\n",
    "llm.invoke(\"Hi\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8ea84",
   "metadata": {},
   "source": [
    "### Initialize the models to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d5584",
   "metadata": {},
   "source": [
    "**We need to define a configuration dictionary to hold the clients for each of our chosen models. This way we can easily swap models and centralizes our model management.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c354f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary will act as our central registry, or \"foundry,\" for all LLM and embedding model clients.\n",
    "llm_config = {\n",
    "    # # For the 'planner', we use Llama 3.1 8B. It's a modern, \n",
    "    # highly capable model that excels at instruction-following.\n",
    "    \"planner\": get_hf_llm(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        model_kwargs={\"temperature\": 0.0}\n",
    "    ),\n",
    "    # For the 'drafter' and 'sql_coder', we use Qwen2 7B. It's a nimble and fast model, perfect for\n",
    "    # tasks like text generation and code completion where speed is valuable.\n",
    "    \"drafter\": get_hf_llm(\n",
    "        model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        model_kwargs={\"temperature\": 0.2}\n",
    "    ),\n",
    "    \"sql_coder\": get_hf_llm(\n",
    "        model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        model_kwargs={\"temperature\": 0.0}\n",
    "    ),\n",
    "    # For the 'director', the highest-level strategic agent, we use the powerful Llama 3 70B model.\n",
    "    # This high-stakes task of diagnosing performance and evolving the system's own procedures\n",
    "    # justifies the use of a larger, more powerful model.\n",
    "    \"director\": get_hf_llm(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        model_kwargs={\"temperature\": 0.0}\n",
    "    ),\n",
    "    # For embeddings, we use 'Qwen/Qwen3-Embedding-8B\",', a top-tier, efficient open-source model.\n",
    "    \"embedding_model\": HuggingFaceEndpointEmbeddings(\n",
    "        model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "        task=\"feature-extraction\",\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f93634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embedding model\n",
    "query = \"Hey that's a great tutorial.\"\n",
    "llm_config[\"embedding_model\"].embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa51219",
   "metadata": {},
   "source": [
    "So we have just created our llm_config dictionary, which serves as a centralized hub for all our model initializations. By assigning different models to different roles, we are creating a cost-performance optimized hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2643a",
   "metadata": {},
   "source": [
    "- **Fast & Nimble (7B-8B models)**: The planner, drafter, and sql_coder roles handle frequent, well-defined tasks. Using smaller models like Qwen2.5 7B and Llama 3.1 8B for these roles ensures low latency and efficient resource usage. They are perfectly capable of following instructions to generate plans, draft text, or write SQL.\n",
    "\n",
    "- **Deep & Strategic (70B model)**: The director agent has the most complex job, it must analyze multi-dimensional performance data and rewrite the entire system operating procedure. This requires deep reasoning and a understanding of cause and effect. For this high-stakes, low-frequency task, we allocate our most powerful resource, the Llama 3 70B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "754a8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM clients configured:\n",
      "Planner (meta-llama/Llama-3.1-8B-Instruct): llm=HuggingFaceEndpoint(temperature=0.0, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' top_p=0.95 max_tokens=512 model_kwargs={}\n",
      "Drafter (Qwen/Qwen2.5-7B-Instruct): llm=HuggingFaceEndpoint(temperature=0.2, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' temperature=0.2 top_p=0.95 max_tokens=512 model_kwargs={}\n",
      "SQL Coder (Qwen/Qwen2.5-7B-Instruct): llm=HuggingFaceEndpoint(temperature=0.0, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' top_p=0.95 max_tokens=512 model_kwargs={}\n",
      "Director (meta-llama/Meta-Llama-3-70B-Instruct): llm=HuggingFaceEndpoint(temperature=0.0, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Meta-Llama-3-70B-Instruct', client=<InferenceClient(model='meta-llama/Meta-Llama-3-70B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Meta-Llama-3-70B-Instruct', timeout=120)>) model_id='meta-llama/Meta-Llama-3-70B-Instruct' top_p=0.95 max_tokens=512 model_kwargs={}\n",
      "Embedding Model (Qwen/Qwen3-Embedding-8B): client=<InferenceClient(model='Qwen/Qwen3-Embedding-8B', timeout=None)> async_client=<InferenceClient(model='Qwen/Qwen3-Embedding-8B', timeout=None)> model='Qwen/Qwen3-Embedding-8B' provider=None repo_id='Qwen/Qwen3-Embedding-8B' task='feature-extraction' model_kwargs=None huggingfacehub_api_token=None\n"
     ]
    }
   ],
   "source": [
    "# Print the configuration to confirm the clients are initialized and their parameters are set correctly.\n",
    "print(\"LLM clients configured:\")\n",
    "print(f\"Planner ({llm_config['planner'].model_id}): {llm_config['planner']}\")\n",
    "print(f\"Drafter ({llm_config['drafter'].model_id}): {llm_config['drafter']}\")\n",
    "print(f\"SQL Coder ({llm_config['sql_coder'].model_id}): {llm_config['sql_coder']}\")\n",
    "print(f\"Director ({llm_config['director'].model_id}): {llm_config['director']}\")\n",
    "print(f\"Embedding Model ({llm_config['embedding_model'].model}): {llm_config['embedding_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f1718",
   "metadata": {},
   "source": [
    "### Preparing the Knowledge Stores\n",
    "\n",
    "The most important part for a RAG pipeline is a rich multi-modal knowledge base to draw upon. A generic, web-based search is not enough for a specialized task like clinical trial design. We need to ground our agents in authoritative, domain-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0bf3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ./data\n",
      "Created directory: ./data/pubmed_articles\n",
      "Created directory: ./data/fda_guidelines\n",
      "Created directory: ./data/ethical_guidelines\n",
      "Created directory: ./data/mimic_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# A dictionary to hold the paths for our different data types. This keeps our file management clean and centralized.\n",
    "data_paths = {\n",
    "    \"base\": \"./data\",\n",
    "    \"pubmed\": \"./data/pubmed_articles\",\n",
    "    \"fda\": \"./data/fda_guidelines\",\n",
    "    \"ethics\": \"./data/ethical_guidelines\",\n",
    "    \"mimic\": \"./data/mimic_db\"\n",
    "}\n",
    "# This loop iterates through our defined paths and uses os.makedirs() to create any directories that don't already exist.\n",
    "# This prevents errors in later steps when we try to save files to these locations.\n",
    "for path in data_paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created directory: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf399f0c",
   "metadata": {},
   "source": [
    "Download Pubmed articles for our medical researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a703212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "\n",
    "def download_pubmed_articles(query, max_articles=20):\n",
    "    \"\"\"Fetches abstracts from PubMed for a given query and saves them as text files.\"\"\"\n",
    "    # The NCBI API requires an email address for identification. We fetch it from our environment variables.\n",
    "    Entrez.email = os.environ.get(\"ENTREZ_EMAIL\")\n",
    "    print(f\"Fetching PubMed articles for query: {query}\")\n",
    "    \n",
    "    # Step 1: Use Entrez.esearch to find the PubMed IDs (PMIDs) for articles matching our query.\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_articles, sort=\"relevance\")\n",
    "    record = Entrez.read(handle)\n",
    "    id_list = record[\"IdList\"]\n",
    "    print(f\"Found {len(id_list)} article IDs.\")\n",
    "    \n",
    "    print(\"Downloading articles...\")\n",
    "    # Step 2: Use Entrez.efetch to retrieve the full records (in MEDLINE format) for the list of PMIDs.\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "    records = Medline.parse(handle)\n",
    "    \n",
    "    count = 0\n",
    "    # Step 3: Iterate through the retrieved records, parse them, and save each abstract to a file.\n",
    "    for i, record in enumerate(records):\n",
    "        pmid = record.get(\"PMID\", \"\")\n",
    "        title = record.get(\"TI\", \"No Title\")\n",
    "        abstract = record.get(\"AB\", \"No Abstract\")\n",
    "        if pmid:\n",
    "            # We name the file after the PMID for easy reference and to avoid duplicates.\n",
    "            filepath = os.path.join(data_paths[\"pubmed\"], f\"{pmid}.txt\")\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(f\"Title: {title}\\n\\nAbstract: {abstract}\")\n",
    "            print(f\"[{i+1}/{len(id_list)}] Fetching PMID: {pmid}... Saved to {filepath}\")\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ed7902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching PubMed articles for query: (SGLT2 inhibitor) AND (type 2 diabetes) AND (renal impairment)\n",
      "Found 20 article IDs.\n",
      "Downloading articles...\n",
      "[1/20] Fetching PMID: 36945734... Saved to ./data/pubmed_articles/36945734.txt\n",
      "[2/20] Fetching PMID: 40470996... Saved to ./data/pubmed_articles/40470996.txt\n",
      "[3/20] Fetching PMID: 38914124... Saved to ./data/pubmed_articles/38914124.txt\n",
      "[4/20] Fetching PMID: 30697905... Saved to ./data/pubmed_articles/30697905.txt\n",
      "[5/20] Fetching PMID: 36335326... Saved to ./data/pubmed_articles/36335326.txt\n",
      "[6/20] Fetching PMID: 36351458... Saved to ./data/pubmed_articles/36351458.txt\n",
      "[7/20] Fetching PMID: 40327845... Saved to ./data/pubmed_articles/40327845.txt\n",
      "[8/20] Fetching PMID: 35113333... Saved to ./data/pubmed_articles/35113333.txt\n",
      "[9/20] Fetching PMID: 34619106... Saved to ./data/pubmed_articles/34619106.txt\n",
      "[10/20] Fetching PMID: 33413348... Saved to ./data/pubmed_articles/33413348.txt\n",
      "[11/20] Fetching PMID: 34272327... Saved to ./data/pubmed_articles/34272327.txt\n",
      "[12/20] Fetching PMID: 34817311... Saved to ./data/pubmed_articles/34817311.txt\n",
      "[13/20] Fetching PMID: 35145275... Saved to ./data/pubmed_articles/35145275.txt\n",
      "[14/20] Fetching PMID: 33878338... Saved to ./data/pubmed_articles/33878338.txt\n",
      "[15/20] Fetching PMID: 38052474... Saved to ./data/pubmed_articles/38052474.txt\n",
      "[16/20] Fetching PMID: 28432726... Saved to ./data/pubmed_articles/28432726.txt\n",
      "[17/20] Fetching PMID: 38913113... Saved to ./data/pubmed_articles/38913113.txt\n",
      "[18/20] Fetching PMID: 31101403... Saved to ./data/pubmed_articles/31101403.txt\n",
      "[19/20] Fetching PMID: 28904068... Saved to ./data/pubmed_articles/28904068.txt\n",
      "[20/20] Fetching PMID: 38684099... Saved to ./data/pubmed_articles/38684099.txt\n",
      "PubMed download complete. 20 articles saved.\n"
     ]
    }
   ],
   "source": [
    "# We define a specific, boolean query to find articles highly relevant to our trial concept.\n",
    "pubmed_query = \"(SGLT2 inhibitor) AND (type 2 diabetes) AND (renal impairment)\"\n",
    "num_downloaded = download_pubmed_articles(pubmed_query)\n",
    "print(f\"PubMed download complete. {num_downloaded} articles saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bdfe2",
   "metadata": {},
   "source": [
    "Now, let’s get the regulatory documents that our Regulatory Specialist agent will need. A key part of trial design is ensuring compliance with government guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6106288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import read\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import io\n",
    "\n",
    "def download_and_extract_text_from_pdf(url, output_path, download=False):\n",
    "    \"\"\"Downloads a PDF from a URL, saves it, and also extracts its text content to a separate .txt file.\"\"\"\n",
    "    print(f\"Downloading FDA Guideline: {url}\")\n",
    "    try:\n",
    "        # We use the 'requests' library to perform the HTTP GET request to download the file.\n",
    "        if download:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status() # This is a good practice that will raise an error if the download fails (e.g., a 404 error).\n",
    "        \n",
    "            # We save the raw PDF file, which is useful for archival purposes.\n",
    "            with open(output_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Successfully downloaded and saved to {output_path}\")\n",
    "        \n",
    "            # We then use pypdf to read the PDF content directly from the in-memory response.\n",
    "            reader = PdfReader(io.BytesIO(response.content))\n",
    "        else:\n",
    "            reader = PdfReader(output_path)\n",
    "        text = \"\"\n",
    "        # We loop through each page of the PDF and append its extracted text.\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "        \n",
    "        # Finally, we save the clean, extracted text to a .txt file. This is the file our RAG system will actually use.\n",
    "        txt_output_path = os.path.splitext(output_path)[0] + '.txt'\n",
    "        with open(txt_output_path, 'w') as f:\n",
    "            f.write(text)\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b48c8",
   "metadata": {},
   "source": [
    "This function, download_and_extract_text_from_pdf, is our tool for handling PDF documents. It's a two-stage process.\n",
    "\n",
    "- First, it downloads and saves the original PDF from the FDA website. Second, and more importantly, it immediately processes that PDF using pypdf to extract all the text content.\n",
    "\n",
    "- It then saves this raw text to a .txt file. This pre-processing step is crucial because it converts the complex PDF format into simple text that our document loaders can easily ingest when we build our vector stores later on.\n",
    "\n",
    "Let’s run the function to download our FDA guidance document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FDA Guideline: https://www.fda.gov/media/71185/download\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This URL points to a real FDA guidance document for developing drugs for diabetes.\n",
    "fda_url = \"https://www.fda.gov/media/71185/download\"\n",
    "fda_pdf_path = os.path.join(data_paths[\"fda\"], \"fda_diabetes_guidance_ocr.pdf\")\n",
    "download_and_extract_text_from_pdf(fda_url, fda_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43ac67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_ecosystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
