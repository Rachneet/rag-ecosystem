{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b52094a",
   "metadata": {},
   "source": [
    "### Building Evaluators from Scratch with LangChain\n",
    "\n",
    "The best way to understand evaluation is to build it. Using basic LangChain components, we can create custom chains that instruct an LLM to act as an impartial ‚Äújudge‚Äù, grading our RAG system‚Äôs output based on criteria we define in a prompt. This gives us maximum control and transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd3911",
   "metadata": {},
   "source": [
    "Let‚Äôs begin with `Correctness`. Our goal is to create a chain that compares the generated_answer to a ground_truth answer and returns a score from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aaf0d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea5c9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hf_llm(model_name):\n",
    "\n",
    "    model = HuggingFaceEndpoint(\n",
    "        model=model_name,\n",
    "        max_new_tokens=1024,\n",
    "        huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "    )\n",
    "\n",
    "    llm = ChatHuggingFace(\n",
    "        llm=model\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = get_hf_llm(\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752c93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import name\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n",
    "\n",
    "output_parser = JsonOutputParser(\n",
    "    name=\"eval_parser\",\n",
    "    pydantic_object=ResultScore\n",
    ")\n",
    "\n",
    "correctness_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Ground Truth: {ground_truth}\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Evaluate the correctness of the generated answer compared to the ground truth.\n",
    "    Score from 0 to 1, where 1 is perfectly correct and 0 is completely incorrect.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"ground_truth\", \"generated_answer\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "correctness_chain = (\n",
    "    correctness_prompt \n",
    "    | llm \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49eeb8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_correctness(question, ground_truth, generated_answer):\n",
    "    \"\"\"A helper function to run our custom correctness evaluation chain.\"\"\"\n",
    "    result = correctness_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"ground_truth\": ground_truth, \n",
    "        \"generated_answer\": generated_answer\n",
    "    })\n",
    "    return result[\"score\"]\n",
    "\n",
    "\n",
    "# Test the correctness chain with a partially correct answer.\n",
    "question = \"What is the capital of France and Spain?\"\n",
    "ground_truth = \"Paris and Madrid\"\n",
    "generated_answer = \"Paris\"\n",
    "score = evaluate_correctness(question, ground_truth, generated_answer)\n",
    "\n",
    "print(f\"Correctness Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fcadb",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "This is arguably more important than correctness for RAG, as it‚Äôs our primary defense against hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4fe34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\", \"generated_answer\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Generated Answer: {generated_answer}\n",
    "\n",
    "    Evaluate if the generated answer to the question can be deduced from the context.\n",
    "    Score of 0 or 1, where 1 is perfectly faithful *AND CAN BE DERIVED FROM THE CONTEXT* and 0 otherwise.\n",
    "    You don't mind if the answer is correct; all you care about is if the answer can be deduced from the context.\n",
    "    \n",
    "    [... a few examples from the notebook to guide the LLM ...]\n",
    "\n",
    "    Example:\n",
    "    Question: What is 2+2?\n",
    "    Context: 4.\n",
    "    Generated Answer: 4.\n",
    "    In this case, the context states '4', but it does not provide information to deduce the answer to 'What is 2+2?', so the score should be 0.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "faithfulness_chain = (\n",
    "    faithfulness_prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18e61347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_faithfulness(question, context, generated_answer):\n",
    "    \"\"\"A helper function to run our custom faithfulness evaluation chain.\"\"\"\n",
    "    result = faithfulness_chain.invoke({\n",
    "        \"question\": question, \n",
    "        \"context\": context, \n",
    "        \"generated_answer\": generated_answer\n",
    "    })\n",
    "    return result[\"score\"]\n",
    "\n",
    "# Test the faithfulness chain. The answer is correct, but is it faithful?\n",
    "question = \"what is 3+3?\"\n",
    "context = \"6\"\n",
    "generated_answer = \"6\"\n",
    "score = evaluate_faithfulness(question, context, generated_answer)\n",
    "\n",
    "print(f\"Faithfulness Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ec06d",
   "metadata": {},
   "source": [
    "This demonstrates the power and precision of a well-defined faithfulness metric. Even though the answer 6 is factually correct, it could not be logically deduced from the provided context ‚Äú6‚Äù.\n",
    "\n",
    "The context didn‚Äôt say 3+3 equals 6. Our system correctly flagged this as an unfaithful answer, which is likely a hallucination where the LLM used its own pre-trained knowledge instead of the provided context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4d746",
   "metadata": {},
   "source": [
    "However, for faster and more robust testing, dedicated evaluation frameworks are the way to go.\n",
    "\n",
    "We‚Äôll explore three popular frameworks: \n",
    "- deepeval, \n",
    "- grouse, and \n",
    "- RAG-specific powerhouse, RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aa7a0",
   "metadata": {},
   "source": [
    "### DeepEval\n",
    "\n",
    "deepeval is a powerful, open-source framework designed to make LLM evaluation simple and intuitive. It provides a set of well-defined metrics that can be easily applied to your RAG pipeline's outputs.\n",
    "\n",
    "The workflow involves creating LLMTestCase objects and measuring them against pre-built metrics like Correctness, Faithfulness, and ContextualRelevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67c9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! üëã How can I help you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inherited class to use my custom llms in deepeval\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "class HFModel(DeepEvalBaseLLM):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        return self.load_model().invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        res = await self.load_model().ainvoke(prompt)\n",
    "        return res.content\n",
    "    \n",
    "    def get_model_name(self, *args, **kwargs):\n",
    "        return super().get_model_name(*args, **kwargs)\n",
    "    \n",
    "custom_llm = HFModel(llm)\n",
    "custom_llm.generate(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae6d08df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Correctness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Correctness [GEval] (score: 0.5, threshold: 0.5, strict: False, evaluation model: None, reason: The actual output correctly identifies Madrid as the capital but omits the explanatory phrase \"is the capital of Spain,\" so it only partially matches the expected content and lacks completeness., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the capital of Spain?\n",
      "  - actual output: MadriD.\n",
      "  - expected output: Madrid is the capital of Spain.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Correctness [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
       "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
       "¬ª \u001b]8;id=650598;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>54s | token cost: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m6.\u001b[0m54s | token cost: \u001b[3;35mNone\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Correctness [GEval]', threshold=0.5, success=True, score=0.5, reason='The actual output correctly identifies Madrid as the capital but omits the explanatory phrase \"is the capital of Spain,\" so it only partially matches the expected content and lacks completeness.', strict_mode=False, evaluation_model=None, error=None, evaluation_cost=None, verbose_logs='Criteria:\\nEvaluate if the actual output matches or closely matches the expected output. If the answer is not correct or complete, reduce score. \\n \\nEvaluation Steps:\\n[\\n    \"Check if Actual Output precisely matches Expected Output.\",\\n    \"If not an exact match, assess whether the content is semantically or functionally equivalent.\",\\n    \"Confirm that all required elements from Expected Output are present in Actual Output.\",\\n    \"Adjust the score based on correctness and completeness.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.5')], conversational=False, multimodal=False, input='What is the capital of Spain?', actual_output='MadriD.', expected_output='Madrid is the capital of Spain.', context=None, retrieval_context=None, turns=None, additional_metadata=None)] confident_link=None test_run_id=None\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "\n",
    "# Create test cases\n",
    "test_case_correctness = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=\"Madrid is the capital of Spain.\",\n",
    "    actual_output=\"MadriD.\"\n",
    ")\n",
    "\n",
    "answer_correctness = GEval(\n",
    "    name=\"Answer Correctness\",\n",
    "    criteria=\"Evaluate if the actual output matches or closely matches the expected output. If the answer is not correct or complete, reduce score.\",\n",
    "    model=custom_llm,\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.INPUT, \n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT, \n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The evaluate() function runs all test cases against all specified metrics\n",
    "evaluation_results = evaluate(\n",
    "    test_cases=[test_case_correctness],\n",
    "    metrics=[answer_correctness]\n",
    ")\n",
    "\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4218398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: None, reason: The score is 1.00 because there are no contradictions to indicate any mismatch between the actual output and the retrieval context., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: what is 3+3?\n",
      "  - actual output: 6\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['6']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
       "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
       "¬ª \u001b]8;id=956630;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>08s | token cost: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span> USD<span style=\"font-weight: bold\">)</span>\n",
       "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m6.\u001b[0m08s | token cost: \u001b[3;35mNone\u001b[0m USD\u001b[1m)\u001b[0m\n",
       "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   ¬ª Pass Rate: \u001b[1;36m100.0\u001b[0m% | Passed: \u001b[1;32m1\u001b[0m | Failed: \u001b[1;31m0\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
       "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions to indicate any mismatch between the actual output and the retrieval context.', strict_mode=False, evaluation_model=None, error=None, evaluation_cost=None, verbose_logs='Truths (limit=None):\\n[\\n    \"The text contains the digit 6.\",\\n    \"6 is an integer.\"\\n] \\n \\nClaims:\\n[\\n    \"The AI output is 6.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": \"The retrieval context states that the text contains the digit 6 and that 6 is an integer, but it does not say that the AI output itself is 6.\"\\n    }\\n]')], conversational=False, multimodal=False, input='what is 3+3?', actual_output='6', expected_output=None, context=None, retrieval_context=['6'], turns=None, additional_metadata=None)] confident_link=None test_run_id=None\n"
     ]
    }
   ],
   "source": [
    "test_case_faithfulness = LLMTestCase(\n",
    "    input=\"what is 3+3?\",\n",
    "    actual_output=\"6\",\n",
    "    retrieval_context=[\"6\"]\n",
    ")\n",
    "\n",
    "answer_faithfulness = FaithfulnessMetric(\n",
    "    model=custom_llm\n",
    ")\n",
    "\n",
    "evaluation_results = evaluate(\n",
    "    test_cases=[test_case_faithfulness],\n",
    "    metrics=[answer_faithfulness]\n",
    ")\n",
    "\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c224be",
   "metadata": {},
   "source": [
    "### Another Powerful Alternative with grouse\n",
    "\n",
    "\n",
    "`grouse` is another excellent open-source option, offering a similar suite of metrics but with a unique focus on allowing deep customization of the \"judge\" prompts. This is useful for fine-tuning evaluation criteria for a specific domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcbf070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='f2f1299ad1659341aa2b59853a27c9cb', created=1764249754, model='openai/gpt-oss-20b', object='chat.completion', system_fingerprint='', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"greeting\":\"Hello\"}', role='assistant', tool_calls=None, function_call=None, reasoning_content='The user says: \"Say hello in JSON format: {\"greeting\": \"...\"}\"\\n\\nThey want presumably a JSON object with a greeting. So we output something like:\\n\\n{\"greeting\": \"Hello\"}\\n\\nThat\\'s it. No extra text.', provider_specific_fields={'reasoning_content': 'The user says: \"Say hello in JSON format: {\"greeting\": \"...\"}\"\\n\\nThey want presumably a JSON object with a greeting. So we output something like:\\n\\n{\"greeting\": \"Hello\"}\\n\\nThat\\'s it. No extra text.'}), provider_specific_fields={})], usage=Usage(completion_tokens=64, prompt_tokens=79, total_tokens=143, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=48, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# Test the model with liellm that grouse uses at the backend\n",
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"huggingface/novita/openai/gpt-oss-20b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello in JSON format: {\\\"greeting\\\": \\\"...\\\"}\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0905f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='{\\n    \"...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n",
      "2025-11-27 14:48:20,642 - LLM Call Tracker - INFO - Cost: 0.0000$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:LLM Call Tracker:Cost: 0.0000$\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouse Faithfulness Score (0 or 1): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from grouse import EvaluationSample, GroundedQAEvaluator\n",
    "import litellm\n",
    "\n",
    "# litellm._turn_on_debug()  # for dubugging\n",
    "litellm.suppress_debug_info = True\n",
    "litellm.set_verbose = False\n",
    "litellm._logging._disable_debugging()  # Internal function to disable debug\n",
    "\n",
    "evaluator = GroundedQAEvaluator(\n",
    "    model_name=\"huggingface/nebius/Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    ")\n",
    "unfaithful_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is located at Rue Rabelais in Paris.\",\n",
    "    expected_output=\"\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"Gustave Eiffel died in his appartment at Rue Rabelais in Paris.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    eval_samples=[unfaithful_sample],\n",
    ").evaluations[0]\n",
    "\n",
    "print(f\"Grouse Faithfulness Score (0 or 1): {result.faithfulness.faithfulness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda7eb9",
   "metadata": {},
   "source": [
    "### Evaluation with RAGAS\n",
    "\n",
    "\n",
    "While deepeval and grouse are great general-purpose evaluators, RAGAS (Retrieval-Augmented Generation Assessment) is a framework built specifically for evaluating RAG pipelines. It provides a comprehensive suite of metrics that measure every component of your system, from retriever to generator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921dbce0",
   "metadata": {},
   "source": [
    "To use RAGAS, we first need to prepare our evaluation data in a specific format. It requires four key pieces of information for each test case:\n",
    "\n",
    "- question: The user's input query.\n",
    "- answer: The final answer generated by our RAG system.\n",
    "- contexts: The list of documents retrieved by our retriever.\n",
    "- ground_truth: The correct, reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c4585f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the evaluation data\n",
    "questions = [\n",
    "    \"What is the name of the three-headed dog guarding the Sorcerer's Stone?\",\n",
    "    \"Who gave Harry Potter his first broomstick?\",\n",
    "    \"Which house did the Sorting Hat initially consider for Harry?\",\n",
    "]\n",
    "\n",
    "# These would be the answers generated by our RAG pipeline\n",
    "generated_answers = [\n",
    "    \"The three-headed dog is named Fluffy.\",\n",
    "    \"Professor McGonagall gave Harry his first broomstick, a Nimbus 2000.\",\n",
    "    \"The Sorting Hat strongly considered putting Harry in Slytherin.\",\n",
    "]\n",
    "\n",
    "# The ground truth, or \"perfect\" answers\n",
    "ground_truth_answers = [\n",
    "    \"Fluffy\",\n",
    "    \"Professor McGonagall\",\n",
    "    \"Slytherin\",\n",
    "]\n",
    "\n",
    "# The context retrieved by our RAG system for each question\n",
    "retrieved_documents = [\n",
    "    [\"A massive, three-headed dog was guarding a trapdoor. Hagrid mentioned its name was Fluffy.\"],\n",
    "    [\"First years are not allowed brooms, but Professor McGonagall, head of Gryffindor, made an exception for Harry.\"],\n",
    "    [\"The Sorting Hat muttered in Harry's ear, 'You could be great, you know, it's all here in your head, and Slytherin will help you on the way to greatness...'\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e3bbb",
   "metadata": {},
   "source": [
    "Next, we structure this data using the Hugging Face datasets library, which RAGAS integrates with seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d18c25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': generated_answers,\n",
    "    'contexts': retrieved_documents,\n",
    "    'ground_truth': ground_truth_answers\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a5bb2",
   "metadata": {},
   "source": [
    "Now, we can define our metrics and run the evaluation. RAGAS offers several powerful, RAG-specific metrics out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3509bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "96f778b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:27<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          user_input  ... answer_correctness\n",
      "0  What is the name of the three-headed dog guard...  ...           0.919079\n",
      "1        Who gave Harry Potter his first broomstick?  ...           0.654729\n",
      "2  Which house did the Sorting Hat initially cons...  ...           0.949443\n",
      "\n",
      "[3 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,       # How factually consistent is the answer with the context? (Prevents hallucination)\n",
    "    answer_relevancy,   # How relevant is the answer to the question?\n",
    "    context_recall,     # Did we retrieve all the necessary context to answer the question?\n",
    "    answer_correctness, # How accurate is the answer compared to the ground truth?\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "llm = get_hf_llm(\"moonshotai/Kimi-K2-Instruct\")\n",
    "eval_llm = LangchainLLMWrapper(llm)\n",
    "eval_embeddings = LangchainEmbeddingsWrapper(hf_embeddings)\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy, \n",
    "    context_recall,\n",
    "    answer_correctness\n",
    "]\n",
    "\n",
    "result = evaluate(\n",
    "    llm=eval_llm,\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    "    embeddings=eval_embeddings\n",
    ")\n",
    "\n",
    "results_df = result.to_pandas()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "317dccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    1.0\n",
       "Name: faithfulness, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"faithfulness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "587c763a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.919079\n",
       "1    0.654729\n",
       "2    0.949443\n",
       "Name: answer_correctness, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"answer_correctness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65a7ab6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "Name: context_recall, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"context_recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce108beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.902707\n",
       "1    0.929892\n",
       "2    0.941209\n",
       "Name: answer_relevancy, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[\"answer_relevancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4cbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_ecosystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
