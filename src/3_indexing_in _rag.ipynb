{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba608fa",
   "metadata": {},
   "source": [
    "## Multi-Representation Indexing\n",
    "\n",
    "The core idea of Multi-Representation Indexing is simple but powerful: instead of embedding the full document chunks, we create a smaller, more focused representation of each chunk (like a summary) and embed that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "206499c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load two different blog posts to create a more diverse knowledge base\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb71f73",
   "metadata": {},
   "source": [
    "Next, we’ll create a chain to generate a summary for each of these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa621b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# initialize llm to use\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = HuggingFaceEndpoint(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    max_new_tokens=1024,\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(\n",
    "    llm=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c597d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachneet/projects/rag_ecosystem/.venv/lib/python3.12/site-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LLM‑Powered Autonomous Agents – Key Concepts & Highlights**\n",
      "\n",
      "| Section | Main Ideas |\n",
      "|---------|------------|\n",
      "| **Agent System Overview** | Large Language Models (LLMs) act as the “brain” of an autonomous agent, complemented by three core modules: (1) **Planning**, (2) **Memory**, and (3) **Tool Use**. |\n",
      "| **Planning** | *Task Decomposition* – chain‑of‑thought (CoT), Tree‑of‑Thoughts, or LLM + P (classical planner).<br>*Self‑Reflection* – ReAct, Reflexion, Chain‑of‑Hindsight, Algorithm Distillation – mechanisms for the agent to critique and improve past actions. |\n",
      "| **Memory** | *Short‑term (STM)* – in‑context learning (limited by model’s context window).<br>*Long‑term (LTM)* – external vector stores (FAISS, HNSW, ScaNN, ANNOY, LSH) that support fast Maximum Inner Product Search (MIPS) for retrieval‑augmented reasoning. |\n",
      "| **Tool Use** | *Neuro‑symbolic routing* – MRKL system (LLM routes queries to expert modules).<br>*Tool‑augmented LLMs* – TALM, Toolformer, ChatGPT plugins, OpenAI function‑calling. <br>*Benchmarks* – API‑Bank evaluates three levels of tool proficiency (call, retrieve, plan). <br>*Frameworks* – HuggingGPT (LLM plans, selects HuggingFace models, executes, summarizes). |\n",
      "| **Case Studies** | • **Scientific Discovery Agents** – ChemCrow (LLM + 13 chemistry tools) and Boiko et al. (autonomous experiment design). <br>• **Generative Agents Simulation** – 25 virtual characters with memory, reflection, and planning, producing emergent social behaviours. |\n",
      "| **Proof‑of‑Concept Examples** | • **AutoGPT** – natural‑language interface with JSON‑formatted commands, self‑criticism, task decomposition.<br>• **GPT‑Engineer** – generates full codebases from natural‑language specs, with step‑by‑step clarification and file‑level code generation. |\n",
      "| **Challenges** | 1. **Finite context length** – limits in‑context memory and formatting reliability.<br>2. **Long‑term planning & robustness** – difficulty adjusting plans after unexpected failures.<br>3. **Reliability of natural‑language interface** – formatting errors, “rebellious” LLM behaviour. |\n",
      "| **References & Citations** | The article cites key works on CoT, Tree‑of‑Thoughts, ReAct, Reflexion, Chain‑of‑Hindsight, MRKL, Toolformer, API‑Bank, ChemCrow, Generative Agents, AutoGPT, GPT‑Engineer, etc. |\n",
      "| **Author & Publication** | Lilian Weng, Lil’Log, 23 Jun 2023 (https://lilianweng.github.io/posts/2023-06-23-agent/). |\n",
      "\n",
      "**Take‑away:**  \n",
      "LLM‑powered autonomous agents combine language reasoning, external memory, and tool‑calling to tackle complex tasks. Current research explores how to decompose goals, reflect on failures, retrieve relevant knowledge, and orchestrate APIs or other models. While demos like AutoGPT and GPT‑Engineer show feasibility, practical deployment still grapples with limited context windows, planning robustness, and the brittleness of natural‑language interfaces.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summary_chain = (\n",
    "    {\"doc\": lambda x: x.page_content} # extract page content from doc\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document: \\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = summary_chain.batch(docs, {\"max_conceurrency\": 5})\n",
    "\n",
    "print(summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e1ba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f1c67",
   "metadata": {},
   "source": [
    "Now comes the crucial part. We need a MultiVectorRetriever which requires two main components:\n",
    "\n",
    "1. A vectorstore to store the embeddings of our summaries.\n",
    "2. A docstore (a simple key-value store) to hold the original, full documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a8aceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.stores import BaseStore, ByteStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import (\n",
    "    CallbackManagerForRetrieverRun,\n",
    "    AsyncCallbackManagerForRetrieverRun,\n",
    ")\n",
    "from langchain_core.stores import InMemoryStore\n",
    "\n",
    "\n",
    "class SearchType(str, Enum):\n",
    "    \"\"\"Enumerator of the types of search to perform.\"\"\"\n",
    "\n",
    "    similarity = \"similarity\"\n",
    "    \"\"\"Similarity search.\"\"\"\n",
    "    similarity_score_threshold = \"similarity_score_threshold\"\n",
    "    \"\"\"Similarity search with a score threshold.\"\"\"\n",
    "    mmr = \"mmr\"\n",
    "    \"\"\"Maximal Marginal Relevance reranking of similarity search.\"\"\"\n",
    "\n",
    "\n",
    "class MultiVectorRetriever(BaseRetriever):\n",
    "    \"\"\"Retrieve from a set of multiple embeddings for the same document.\"\"\"\n",
    "\n",
    "    vectorstore: VectorStore\n",
    "    \"\"\"The underlying vectorstore to use to store small chunks\n",
    "    and their embedding vectors\"\"\"\n",
    "    byte_store: Optional[ByteStore] = None\n",
    "    \"\"\"The lower-level backing storage layer for the parent documents\"\"\"\n",
    "    docstore: BaseStore[str, Document]\n",
    "    \"\"\"The storage interface for the parent documents\"\"\"\n",
    "    id_key: str = \"doc_id\"\n",
    "    search_kwargs: dict = Field(default_factory=dict)\n",
    "    \"\"\"Keyword arguments to pass to the search function.\"\"\"\n",
    "    search_type: SearchType = SearchType.similarity\n",
    "    \"\"\"Type of search to perform (similarity / mmr)\"\"\"\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def shim_docstore(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate and set up the docstore from byte_store if needed.\"\"\"\n",
    "        byte_store = values.get(\"byte_store\")\n",
    "        docstore = values.get(\"docstore\")\n",
    "        if docstore is None:\n",
    "            if byte_store is not None:\n",
    "                # Use byte_store directly as docstore\n",
    "                docstore = byte_store\n",
    "            else:\n",
    "                # Create a default InMemoryStore\n",
    "                docstore = InMemoryStore()\n",
    "        values[\"docstore\"] = docstore\n",
    "        return values\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        if self.search_type == SearchType.mmr:\n",
    "            sub_docs = self.vectorstore.max_marginal_relevance_search(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "        elif self.search_type == SearchType.similarity_score_threshold:\n",
    "            sub_docs_and_similarities = (\n",
    "                self.vectorstore.similarity_search_with_relevance_scores(\n",
    "                    query, **self.search_kwargs\n",
    "                )\n",
    "            )\n",
    "            sub_docs = [sub_doc for sub_doc, _ in sub_docs_and_similarities]\n",
    "        else:\n",
    "            sub_docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n",
    "\n",
    "        # We do this to maintain the order of the ids that are returned\n",
    "        ids = []\n",
    "        for d in sub_docs:\n",
    "            if self.id_key in d.metadata and d.metadata[self.id_key] not in ids:\n",
    "                ids.append(d.metadata[self.id_key])\n",
    "        docs = self.docstore.mget(ids)\n",
    "        return [d for d in docs if d is not None]\n",
    "\n",
    "    async def _aget_relevant_documents(\n",
    "        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Asynchronously get documents relevant to a query.\n",
    "        Args:\n",
    "            query: String to find relevant documents for\n",
    "            run_manager: The callbacks handler to use\n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        if self.search_type == SearchType.mmr:\n",
    "            sub_docs = await self.vectorstore.amax_marginal_relevance_search(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "        elif self.search_type == SearchType.similarity_score_threshold:\n",
    "            sub_docs_and_similarities = (\n",
    "                await self.vectorstore.asimilarity_search_with_relevance_scores(\n",
    "                    query, **self.search_kwargs\n",
    "                )\n",
    "            )\n",
    "            sub_docs = [sub_doc for sub_doc, _ in sub_docs_and_similarities]\n",
    "        else:\n",
    "            sub_docs = await self.vectorstore.asimilarity_search(\n",
    "                query, **self.search_kwargs\n",
    "            )\n",
    "\n",
    "        # We do this to maintain the order of the ids that are returned\n",
    "        ids = []\n",
    "        for d in sub_docs:\n",
    "            if self.id_key in d.metadata and d.metadata[self.id_key] not in ids:\n",
    "                ids.append(d.metadata[self.id_key])\n",
    "        docs = await self.docstore.amget(ids)\n",
    "        return [d for d in docs if d is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72ee865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.stores import InMemoryByteStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"summaries\",\n",
    "    embedding_function=hf_embeddings,\n",
    ")\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"  # will link sumamries to parent doc ids\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Generate unique IDs for each of our original documents\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Create new Document objects for the summaries, adding the 'doc_id' to their metadata\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add the summaries to the vectorstore\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Add the original documents to the docstore, linking them by the same IDs\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1be34905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with summaries\n",
      "**LLM‑Powered Autonomous Agents – Key Concepts & Highlights**\n",
      "\n",
      "| Section | Main Ideas |\n",
      "|---------|------------|\n",
      "| **Agent System Overview** | Large Language Models (LLMs) act as the “brain” of an autonomous agent, complemented by three core modules: (1) **Planning**, (2) **Memory**, and (3) **Tool Use**. |\n",
      "| **Planning** | *Task Decomposition* – chain‑of‑thought (CoT), Tree‑of‑Thoughts, or LLM + P (classical planner).<br>*Self‑Reflection* – ReAct, Reflexion, Chain‑of‑Hindsight, Algorithm Distillation – mechanisms for the agent to critique and improve past actions. |\n",
      "| **Memory** | *Short‑term (STM)* – in‑context learning (limited by model’s context window).<br>*Long‑term (LTM)* – external vector stores (FAISS, HNSW, ScaNN, ANNOY, LSH) that support fast Maximum Inner Product Search (MIPS) for retrieval‑augmented reasoning. |\n",
      "| **Tool Use** | *Neuro‑symbolic routing* – MRKL system (LLM routes queries to expert modules).<br>*Tool‑augmented LLMs* – TALM, Toolformer, ChatGPT plugins, OpenAI function‑calling. <br>*Benchmarks* – API‑Bank evaluates three levels of tool proficiency (call, retrieve, plan). <br>*Frameworks* – HuggingGPT (LLM plans, selects HuggingFace models, executes, summarizes). |\n",
      "| **Case Studies** | • **Scientific Discovery Agents** – ChemCrow (LLM + 13 chemistry tools) and Boiko et al. (autonomous experiment design). <br>• **Generative Agents Simulation** – 25 virtual characters with memory, reflection, and planning, producing emergent social behaviours. |\n",
      "| **Proof‑of‑Concept Examples** | • **AutoGPT** – natural‑language interface with JSON‑formatted commands, self‑criticism, task decomposition.<br>• **GPT‑Engineer** – generates full codebases from natural‑language specs, with step‑by‑step clarification and file‑level code generation. |\n",
      "| **Challenges** | 1. **Finite context length** – limits in‑context memory and formatting reliability.<br>2. **Long‑term planning & robustness** – difficulty adjusting plans after unexpected failures.<br>3. **Reliability of natural‑language interface** – formatting errors, “rebellious” LLM behaviour. |\n",
      "| **References & Citations** | The article cites key works on CoT, Tree‑of‑Thoughts, ReAct, Reflexion, Chain‑of‑Hindsight, MRKL, Toolformer, API‑Bank, ChemCrow, Generative Agents, AutoGPT, GPT‑Engineer, etc. |\n",
      "| **Author & Publication** | Lilian Weng, Lil’Log, 23 Jun 2023 (https://lilianweng.github.io/posts/2023-06-23-agent/). |\n",
      "\n",
      "**Take‑away:**  \n",
      "LLM‑powered autonomous agents combine language reasoning, external memory, and tool‑calling to tackle complex tasks. Current research explores how to decompose goals, reflect on failures, retrieve relevant knowledge, and orchestrate APIs or other models. While demos like AutoGPT and GPT‑Engineer show feasibility, practical deployment still grapples with limited context windows, planning robustness, and the brittleness of natural‑language interfaces.\n"
     ]
    }
   ],
   "source": [
    "query = \"memory in agents\"\n",
    "\n",
    "sub_docs = vector_store.similarity_search(query, k=1)\n",
    "print(\"Searching with summaries\")\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51be9e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metadata showing the link to the parent document ---\n",
      "{'doc_id': '4ca91542-36ad-4941-ae19-79afbf7b47aa'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Metadata showing the link to the parent document ---\")\n",
    "print(sub_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1880e",
   "metadata": {},
   "source": [
    "As you can see, the search found the summary that mentions “memory.” Now, the MultiVectorRetriever will use the doc_id from this summary's metadata to automatically fetch the full parent document from the docstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76f83527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The full document retrieved by the MultiVectorRetriever ---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three:\n"
     ]
    }
   ],
   "source": [
    "# Let the full retriever do its job\n",
    "retrieved_docs = retriever.invoke(query, n_results=1)\n",
    "\n",
    "# Print the beginning of the retrieved full document\n",
    "print(\"\\n--- The full document retrieved by the MultiVectorRetriever ---\")\n",
    "print(retrieved_docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4d7c2",
   "metadata": {},
   "source": [
    "### Hierarchical Indexing (RAPTOR) Knowledge Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ecd20",
   "metadata": {},
   "source": [
    "The Theory: RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) \n",
    "\n",
    "It takes the multi-representation idea a step further. Instead of just one layer of summaries, RAPTOR builds a multi-level tree of summaries. It starts by clustering small document chunks. It then summarizes each cluster.\n",
    "\n",
    "Then, it takes these summaries, clusters them, and summarizes the new clusters. This process repeats, creating a hierarchy of knowledge from fine-grained details to high-level concepts. When you query, you can search at different levels of this tree, allowing for retrieval that can be as specific or as general as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90aaf7",
   "metadata": {},
   "source": [
    "### Token-level Precision (ColBert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf839ec0",
   "metadata": {},
   "source": [
    "ColBERT (Contextualized Late Interaction over BERT) offers a more granular approach. It generates a separate, context-aware embedding for every single token in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43515709",
   "metadata": {},
   "source": [
    "Now, let’s index a Wikipedia page using ColBERT’s unique token-level approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_ecosystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
